{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b586ff0d",
   "metadata": {},
   "source": [
    "1. Explain the concept of forward propagation in a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846c834",
   "metadata": {},
   "source": [
    "**Forward Propagation in a Neural Network**\n",
    "\n",
    "Forward propagation is the process by which input data passes through a neural network to produce an output. It is the first phase of training (or prediction) where each neuron's output is computed based on its inputs and the weights.\n",
    "\n",
    "**Step-by-Step Process**\n",
    "\n",
    "    Let‚Äôs say a neural network has:\n",
    "\n",
    "Input layer\n",
    "\n",
    "One or more hidden layers\n",
    "\n",
    "Output layer\n",
    "\n",
    "    Each layer consists of neurons (units), and each connection has a weight and possibly a bias.\n",
    "\n",
    "**Mathematical Formulation**\n",
    "\n",
    "    For each neuron in layer l:\n",
    "\n",
    "1. Weighted Sum (Linear Transformation):\n",
    "\n",
    "z[l]=W[l]‚ãÖa[l‚àí1]+b[l]\n",
    " \n",
    "W[l] : weights of layer \n",
    "\n",
    "a[l‚àí1] : activations (outputs) from previous layer\n",
    "\n",
    "b[l]: bias of layer \n",
    "\n",
    "2. Activation Function:\n",
    "\n",
    "a[l]=f(z[l])\n",
    "\n",
    "f: non-linear activation (e.g., ReLU, sigmoid, tanh)\n",
    "\n",
    "    You repeat this from the input layer through the hidden layers to the output layer.\n",
    "\n",
    "**Example (Simple Network)**\n",
    "\n",
    "Input Layer ‚Üí Hidden Layer (ReLU) ‚Üí Output Layer (Sigmoid)\n",
    "\n",
    "    Given input:\n",
    "\n",
    "X=[x1,x2]\n",
    "\n",
    "Hidden Layer:\n",
    "\n",
    "z[1] = W[1]X+b[1]\n",
    " \n",
    "ùëé[1] = ReLU(ùëß[1])\n",
    "\n",
    "Output Layer:\n",
    "\n",
    "z[2] = W[2]a[1]+b[2]\n",
    " \n",
    "ùë¶^ = Sigmoid(z[2])\n",
    "\n",
    "**Purpose of Forward Propagation**\n",
    "\n",
    "In training: to calculate the predicted output y^ and the loss (how far prediction is from the true output).\n",
    "\n",
    "In inference: to make predictions using learned weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc43ce1",
   "metadata": {},
   "source": [
    "2. What is the purpose of the activation function in forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8450b1e",
   "metadata": {},
   "source": [
    "The activation function is a crucial component in forward propagation of a neural network. It introduces non-linearity into the network, allowing it to learn complex patterns and make meaningful predictions.\n",
    "\n",
    "**Why Activation Functions Are Important**\n",
    "\n",
    "1. Introduce Non-Linearity\n",
    "\n",
    "Without activation functions, each layer in a neural network would just perform linear transformations.\n",
    "\n",
    "A stack of linear layers is still just a linear function, regardless of depth.\n",
    "\n",
    "Real-world data is often non-linear, so we need activation functions to model non-linear relationships.\n",
    "\n",
    "2. Allow Learning of Complex Representations\n",
    "\n",
    "    Non-linear activation functions allow the network to learn:\n",
    "\n",
    "Interactions between features\n",
    "\n",
    "Hierarchical patterns (especially in deep networks like CNNs, RNNs)\n",
    "\n",
    "3. Enable Deep Learning\n",
    "\n",
    "Deep networks rely on non-linear activations to create layer-wise abstractions.\n",
    "\n",
    "Each hidden layer transforms the data into a more meaningful representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b9548",
   "metadata": {},
   "source": [
    "**Common Activation Functions**\n",
    "\n",
    "| Activation  | Formula                             | Characteristics                                                       |\n",
    "| ----------- | ----------------------------------- | --------------------------------------------------------------------- |\n",
    "| **ReLU**    | $\\max(0, x)$                        | Fast, avoids vanishing gradients                                      |\n",
    "| **Sigmoid** | $\\frac{1}{1 + e^{-x}}$              | Outputs in (0,1); good for binary classification                      |\n",
    "| **Tanh**    | $\\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | Outputs in (-1,1); zero-centered                                      |\n",
    "| **Softmax** | $\\frac{e^{x_i}}{\\sum_j e^{x_j}}$    | Converts scores to probabilities (used in multi-class classification) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e96ce",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "    Suppose:\n",
    "\n",
    "z=W‚ãÖx+b=2.5\n",
    "\n",
    "1. Linear Output:\n",
    "\n",
    "Without activation:\n",
    "\n",
    "a=z=2.5(linear)\n",
    "\n",
    "\n",
    "2. With ReLU:\n",
    "\n",
    "a=max(0,2.5)=2.5\n",
    "\n",
    "3. With Sigmoid:\n",
    "\n",
    "a = 1/1+e^‚àí2.5 ‚âà 0.92\n",
    "\n",
    "Each activation changes the output and affects how the network learns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdafd19",
   "metadata": {},
   "source": [
    "3.  Describe the steps involved in the backward propagation (backpropagation) algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2926531a",
   "metadata": {},
   "source": [
    "**Steps Involved in the Backward Propagation (Backpropagation) Algorithm**\n",
    "\n",
    "Backpropagation is the core algorithm used to train neural networks. It is the process of computing gradients of the loss function with respect to the weights and biases, and then updating the parameters using those gradients to minimize the loss.\n",
    "\n",
    "**Overall Training Loop:**\n",
    "\n",
    "1. Forward Propagation ‚Üí Compute predictions\n",
    "\n",
    "2. Compute Loss ‚Üí Measure prediction error\n",
    "\n",
    "3. Backward Propagation ‚Üí Compute gradients\n",
    "\n",
    "4. Update Weights ‚Üí Use gradient descent\n",
    "\n",
    "**Steps in Backpropagation (Layer by Layer)**\n",
    "\n",
    "Let‚Äôs assume a neural network with L layers.\n",
    "\n",
    "  Notation:\n",
    "  \n",
    "a[l] : activation/output of layer l\n",
    "\n",
    "z[l] : pre-activation (z=Wa^[l‚àí1]+b)\n",
    "\n",
    "W^[l],b^[l] : weights and biases\n",
    "\n",
    "y^ : predicted output\n",
    "\n",
    "y: true label\n",
    "\n",
    "L: loss function (e.g., MSE, cross-entropy)\n",
    "\n",
    "f[l] : activation function of layer \n",
    "\n",
    "\n",
    "**Step-by-Step Breakdown**\n",
    "\n",
    "  Step 1: Compute Loss\n",
    "\n",
    "Calculate the loss between the predicted output y^ and the true value y:\n",
    "\n",
    "L=Loss(y^,y)\n",
    "\n",
    "  Step 2: Compute Derivative of Loss w.r.t. Output Activation\n",
    "\n",
    "For the output layer L:\n",
    "\n",
    "Œ¥[L]\n",
    " = ‚àÇL/‚àÇa[L]‚ãÖf‚Ä≤[L](z[L])\n",
    "\n",
    "This is the gradient of the loss with respect to the weighted input at the output layer.\n",
    "\n",
    "  Step 3: Backpropagate Error Through Layers\n",
    "\n",
    "For each layer l from L to 1:\n",
    "\n",
    "1. Compute gradient of loss w.r.t. weights:\n",
    "\n",
    "‚àÇL/‚àÇW[l] = Œ¥[l]‚ãÖ(a[l‚àí1])T\n",
    " \n",
    "2. Compute gradient of loss w.r.t. biases:\n",
    "\n",
    "‚àÇL/‚àÇb[l] = Œ¥[l]\n",
    " \n",
    "3. Propagate error to previous layer:\n",
    "\n",
    "Œ¥[l‚àí1] = (W[l])T‚ãÖŒ¥[l]‚ãÖf‚Ä≤[l‚àí1](z[l‚àí1])\n",
    "\n",
    "Step 4: Update Parameters (Gradient Descent)\n",
    "\n",
    "W[l] :=W[l]‚àíŒ±‚ãÖ ‚àÇL/‚àÇW[l]\n",
    " \n",
    "\n",
    "b[l] :=b[l]‚àíŒ±‚ãÖ‚àÇL/‚àÇb[l]\n",
    " \n",
    "Œ± is the learning rate.\n",
    "\n",
    "**Visual Overview**\n",
    "\n",
    "Forward Propagation ‚Üí\n",
    "    Input ‚Üí Hidden Layers ‚Üí Output ‚Üí Compute Loss\n",
    "\n",
    "Backward Propagation ‚Üê\n",
    "\n",
    "    Output Layer: Œ¥\n",
    "\n",
    "    Hidden Layers: Œ¥ propagates backward\n",
    "\n",
    "    Compute Gradients: ‚àÇLoss/‚àÇW, ‚àÇLoss/‚àÇb\n",
    "    \n",
    "    Update: W := W - Œ± * ‚àÇLoss/‚àÇW\n",
    "\n",
    "\n",
    "**Example (Simplified)**\n",
    "\n",
    "Assume one hidden layer with sigmoid:\n",
    "\n",
    "1. z^[1]=W^[1]x+b^[1],ùëé^[1]=ùúé(ùëß^[1])\n",
    "\n",
    "2. z^[2]=W^[2]a^[1]+b^[2],y^=œÉ(z^[2])\n",
    "\n",
    "Then during backprop:\n",
    "\n",
    "Compute Œ¥^[2]=(y^‚àíy)‚ãÖœÉ‚Ä≤(z^[2])\n",
    "\n",
    "Then Œ¥^[1]=(W^[2])^TŒ¥^[2]‚ãÖœÉ‚Ä≤(z^[1])\n",
    "\n",
    "Finally update weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642102d",
   "metadata": {},
   "source": [
    "**Summary Table**\n",
    "\n",
    "| Step | Description                              |\n",
    "| ---- | ---------------------------------------- |\n",
    "| 1    | Compute prediction using forward pass    |\n",
    "| 2    | Calculate loss                           |\n",
    "| 3    | Compute error at output layer            |\n",
    "| 4    | Propagate error backward                 |\n",
    "| 5    | Compute gradients for weights and biases |\n",
    "| 6    | Update weights using gradient descent    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce31fab",
   "metadata": {},
   "source": [
    "4. What is the purpose of the chain rule in backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655dcfa9",
   "metadata": {},
   "source": [
    "**Purpose of the Chain Rule in Backpropagation**\n",
    "\n",
    "The chain rule is the mathematical foundation of backpropagation. It allows us to efficiently compute gradients of the loss function with respect to each weight and bias in a multi-layer neural network.\n",
    "\n",
    "**Why Is the Chain Rule Needed?**\n",
    "\n",
    "In neural networks, outputs depend on multiple layers of nested functions. To train the network, we need to know how a change in each parameter affects the loss.\n",
    "\n",
    "Backpropagation uses the chain rule from calculus to break down this complex dependency into smaller, manageable steps.\n",
    "\n",
    "**What Is the Chain Rule?**\n",
    "\n",
    "    If a variable z depends on y, which depends on x, then:\n",
    "\n",
    "dz/dx = dz/dy‚ãÖdy/dx\n",
    "‚Äã\n",
    " \n",
    "    In a neural network, the loss L depends on:\n",
    "\n",
    "The output, which depends on:\n",
    "\n",
    "The activations, which depend on:\n",
    "\n",
    "The weights, biases, and inputs.\n",
    "\n",
    "    So to compute ‚àÇL/‚àÇW, we apply:\n",
    "\n",
    "‚àÇL/‚àÇW = ‚àÇL/‚àÇa ‚ãÖ ‚àÇa/‚àÇz ‚ãÖ ‚àÇz/‚àÇW\n",
    "\n",
    "‚Äã\n",
    " \n",
    "**How the Chain Rule Works in Backpropagation**\n",
    "\n",
    "    Let‚Äôs say:\n",
    "\n",
    "z^[l]=W^[l]a^[l‚àí1]+^b[l]\n",
    "\n",
    "a^[l]=f(z^[l])\n",
    "\n",
    "L=Loss(a^[L],y)\n",
    "\n",
    "\n",
    "To find the gradient ‚àÇL/‚àÇW^[l], we apply the chain rule:\n",
    "\n",
    "‚àÇL/‚àÇW^[l] = ‚àÇL/‚àÇa^[l] ‚ãÖ ‚àÇa^[l]/‚àÇz^[l] ‚ãÖ ‚àÇz^[l]/‚àÇW^[l]\n",
    " \n",
    "‚Äã\n",
    " \n",
    "Each term is easier to compute, and this process is repeated layer by layer, from output back to the input.\n",
    "\n",
    "**Why It‚Äôs So Powerful**\n",
    "\n",
    "Neural networks can have millions of parameters and deep architectures.\n",
    "\n",
    "The chain rule allows us to reuse intermediate computations during the backward pass, making training efficient and scalable.\n",
    "\n",
    "It forms the basis of automatic differentiation used in PyTorch, TensorFlow, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a732f",
   "metadata": {},
   "source": [
    "5.  Implement the forward propagation process for a simple neural network with one hidden layer using \n",
    "NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8213fa",
   "metadata": {},
   "source": [
    "**Network Architecture:**\n",
    "\n",
    "Input layer: 2 features\n",
    "\n",
    "Hidden layer: 2 neurons (activation: ReLU)\n",
    "\n",
    "Output layer: 1 neuron (activation: Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf834c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X:\n",
      " [[ 0.5 -0.2]]\n",
      "Z1 (input to hidden):\n",
      " [[-0.11533401 -0.60787508]]\n",
      "A1 (hidden activation - ReLU):\n",
      " [[0. 0.]]\n",
      "Z2 (input to output):\n",
      " [[-0.23415337 -0.23413696]]\n",
      "A2 (output - Sigmoid):\n",
      " [[0.44172766 0.44173171]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "# Input data (1 sample, 2 features)\n",
    "X = np.array([[0.5,-0.2]])\n",
    "\n",
    "# Weights and biases initialization\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(2,2) #(input_size, hidden_size)\n",
    "b1 = np.random.randn(1,2) #(1,hidden_size)\n",
    "\n",
    "W2 = np.random.randn(2,1) # (hidden_size, output_size)\n",
    "b2 = np.random.randn(1,1) # (1, output_size)\n",
    "\n",
    "# Forward propagation\n",
    "# Layer 1 (hidden layer)\n",
    "Z1 = np.dot(X,W1) + b1 # Linear step\n",
    "A1 = relu(Z1) # Activation (ReLU)\n",
    "\n",
    "\n",
    "# Layer 2 (output layer)\n",
    "Z2 = np.dot(A1,W2) + b1 # Linear step\n",
    "A2 = sigmoid(Z2) # Activation (Sigmoid)\n",
    "\n",
    "# Print results\n",
    "print(\"Input X:\\n\",X)\n",
    "print(\"Z1 (input to hidden):\\n\",Z1)\n",
    "print(\"A1 (hidden activation - ReLU):\\n\",A1)\n",
    "print(\"Z2 (input to output):\\n\",Z2)\n",
    "print(\"A2 (output - Sigmoid):\\n\",A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e345b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
