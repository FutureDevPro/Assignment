{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd5eb8c",
   "metadata": {},
   "source": [
    "1. What is clustering in machine learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976c013",
   "metadata": {},
   "source": [
    "Clustering is an unsupervised learning technique used to group similar data points into clusters based on features or patterns. The goal is to ensure data points in the same cluster are more similar to each other than to those in other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c2618",
   "metadata": {},
   "source": [
    "2. Explain the difference between supervised and unsupervised clustering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7295bf20",
   "metadata": {},
   "source": [
    "Supervised learning uses labeled data; clustering is not supervised.\n",
    "\n",
    "Clustering is a form of unsupervised learning, meaning:\n",
    "\n",
    "There are no predefined labels.\n",
    "\n",
    "The model identifies natural groupings or structures in the data.\n",
    "\n",
    "Note: “Supervised clustering” is a misleading term. Sometimes, semi-supervised approaches involve some labels, but traditional clustering is fully unsupervised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f9051",
   "metadata": {},
   "source": [
    "3. What are the key applications of clustering algorithms \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41c685",
   "metadata": {},
   "source": [
    "Customer segmentation (e.g., in marketing)\n",
    "\n",
    "Image compression and segmentation\n",
    "\n",
    "Document or news classification\n",
    "\n",
    "Anomaly detection\n",
    "\n",
    "Social network analysis\n",
    "\n",
    "Recommendation systems\n",
    "\n",
    "Genomic data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aaa522",
   "metadata": {},
   "source": [
    "4. Describe the K-means clustering algorithm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2e9c2",
   "metadata": {},
   "source": [
    "K-Means aims to partition n observations into k clusters by minimizing the within-cluster variance.\n",
    "\n",
    "    Steps:\n",
    "\n",
    "Choose the number of clusters k.\n",
    "\n",
    "Initialize k centroids randomly.\n",
    "\n",
    "Assign each data point to the nearest centroid (using Euclidean distance).\n",
    "\n",
    "Recompute centroids by averaging points in each cluster.\n",
    "\n",
    "Repeat steps 3–4 until convergence (centroids no longer move or a max iteration is reached)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8bcad",
   "metadata": {},
   "source": [
    "5. What are the main advantages and disadvantages of K-means clustering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabb227",
   "metadata": {},
   "source": [
    "    Advantages:\n",
    "\n",
    "Simple and easy to implement\n",
    "\n",
    "Fast and efficient for large datasets\n",
    "\n",
    "Works well with spherical, equally sized clusters\n",
    "\n",
    "    Disadvantages:\n",
    "\n",
    "Need to specify k in advance\n",
    "\n",
    "Sensitive to initial centroids (can lead to different results)\n",
    "\n",
    "Poor performance with non-spherical or varying density clusters\n",
    "\n",
    "Affected by outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05668880",
   "metadata": {},
   "source": [
    "6. How does hierarchical clustering work \n",
    "|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcc16e",
   "metadata": {},
   "source": [
    "    Hierarchical clustering builds a tree of clusters (dendrogram). Two types:\n",
    "\n",
    "Agglomerative (bottom-up): Start with each data point as a cluster and merge the closest pair step-by-step.\n",
    "\n",
    "Divisive (top-down): Start with one large cluster and divide it into smaller ones.\n",
    "\n",
    "    No need to predefine the number of clusters—cutting the dendrogram at a desired level gives the final clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda64a8e",
   "metadata": {},
   "source": [
    "7. What are the different linkage criteria used in hierarchical clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b81cf9",
   "metadata": {},
   "source": [
    "    Linkage defines the distance between clusters:\n",
    "\n",
    "Single linkage: Minimum distance between points in the two clusters\n",
    "\n",
    "Complete linkage: Maximum distance between points\n",
    "\n",
    "Average linkage: Mean distance between all points in the two clusters\n",
    "\n",
    "Ward’s method: Minimizes total within-cluster variance (often preferred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba62a5f7",
   "metadata": {},
   "source": [
    "8. Explain the concept of DBSCAN clustering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac5138",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that forms clusters based on density of points.\n",
    "\n",
    "    Concepts:\n",
    "\n",
    "Core points: Have enough neighboring points within a radius\n",
    "\n",
    "Border points: Near core points but don’t have enough neighbors\n",
    "\n",
    "Noise points: Do not belong to any cluster\n",
    "\n",
    "    Clusters are formed by connecting density-reachable points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61192f",
   "metadata": {},
   "source": [
    "9. What are the parameters involved in DBSCAN clustering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00825e3",
   "metadata": {},
   "source": [
    "    eps: Radius of neighborhood to search around a point\n",
    "\n",
    "    min_samples: Minimum number of points required to form a dense region\n",
    "\n",
    "These control how clusters are defined and how noise is handled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed69163",
   "metadata": {},
   "source": [
    "10. Describe the process of evaluating clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a001c13f",
   "metadata": {},
   "source": [
    "Since clustering is unsupervised, evaluation doesn’t rely on ground truth labels (though some datasets have labels for comparison).\n",
    "\n",
    "    Internal Evaluation Metrics:\n",
    "\n",
    "Silhouette Score: Measures how similar a point is to its cluster vs. others.\n",
    "\n",
    "Davies-Bouldin Index: Lower is better (compact & well-separated clusters).\n",
    "\n",
    "Inertia (for K-Means): Sum of squared distances to nearest cluster center.\n",
    "\n",
    "    External Metrics (if true labels are available):\n",
    "\n",
    "Adjusted Rand Index (ARI)\n",
    "\n",
    "Normalized Mutual Information (NMI)\n",
    "\n",
    "Fowlkes–Mallows Index\n",
    "\n",
    "    Visual Tools:\n",
    "\n",
    "PCA or t-SNE for 2D visualization of high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c17f07",
   "metadata": {},
   "source": [
    "11. What is the silhouette score, and how is it calculated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d90bd7",
   "metadata": {},
   "source": [
    "The Silhouette Score measures how well a point fits within its cluster compared to other clusters.\n",
    "\n",
    "    For a single point:\n",
    "\n",
    "a= average distance to all other points in the same cluster\n",
    "\n",
    "b= average distance to all points in the nearest cluster\n",
    "\n",
    "    Silhouette score for that point:\n",
    "\n",
    "s=b−a/max(a,b)\n",
    "\n",
    "    ​Ranges from -1 to 1:\n",
    "\n",
    "Close to 1 → well clustered\n",
    "\n",
    "Close to 0 → overlapping clusters\n",
    "\n",
    "Negative → wrongly clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c234f23",
   "metadata": {},
   "source": [
    "12. Discuss the challenges of clustering high-dimensional data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53183464",
   "metadata": {},
   "source": [
    "Curse of dimensionality: Distances become less meaningful as dimensions increase.\n",
    "\n",
    "Sparsity: Data becomes sparse, reducing density-based algorithms' effectiveness.\n",
    "\n",
    "Computational complexity: More features = more computation.\n",
    "\n",
    "Visualization difficulty: Hard to interpret or evaluate clusters visually.\n",
    "\n",
    "Irrelevant features: Can dominate clustering; feature selection or dimensionality reduction (PCA, t-SNE, UMAP) becomes essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f1871",
   "metadata": {},
   "source": [
    "13. Explain the concept of density-based clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a389e",
   "metadata": {},
   "source": [
    "Density-based clustering groups data based on regions of high point density separated by regions of low density.\n",
    "\n",
    "    Key features:\n",
    "\n",
    "Can find arbitrarily shaped clusters\n",
    "\n",
    "Can automatically detect noise/outliers\n",
    "\n",
    "Popular algorithm: DBSCAN\n",
    "\n",
    "    Idea: A cluster is formed if a core point has enough nearby points (within eps), and other points are reachable from it via a chain of neighboring core points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366b305",
   "metadata": {},
   "source": [
    "14. How does Gaussian Mixture Model (GMM) clustering differ from K-means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851cd6b",
   "metadata": {},
   "source": [
    "| Feature       | K-Means                    | GMM Clustering                         |\n",
    "| ------------- | -------------------------- | -------------------------------------- |\n",
    "| Cluster shape | Circular/Spherical         | Elliptical / Arbitrary shape           |\n",
    "| Membership    | Hard (1 point → 1 cluster) | Soft (probabilistic)                   |\n",
    "| Based on      | Euclidean distance         | Probability density estimation         |\n",
    "| Model         | Non-probabilistic          | Probabilistic (Gaussian distributions) |\n",
    "| Better for    | Equal-sized clusters       | Overlapping clusters                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ecf3c",
   "metadata": {},
   "source": [
    "15. What are the limitations of traditional clustering algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af0116",
   "metadata": {},
   "source": [
    "Require manual tuning of parameters (e.g., k in K-means)\n",
    "\n",
    "Sensitive to outliers and scaling\n",
    "\n",
    "Assume specific cluster shapes (e.g., spherical in K-means)\n",
    "\n",
    "Poor performance on imbalanced or non-convex clusters\n",
    "\n",
    "Don’t work well with categorical data (unless encoded)\n",
    "\n",
    "Limited by distance metrics in high dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5a855",
   "metadata": {},
   "source": [
    "16. Discuss the applications of spectral clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58acb004",
   "metadata": {},
   "source": [
    "Spectral clustering uses eigenvalues of the similarity matrix of the data to reduce dimensions before clustering.\n",
    "\n",
    "    Applications:\n",
    "\n",
    "Image segmentation\n",
    "\n",
    "Social network analysis\n",
    "\n",
    "Speech and signal processing\n",
    "\n",
    "Community detection\n",
    "\n",
    "Non-convex cluster separation\n",
    "\n",
    "Document and text clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e5ff7",
   "metadata": {},
   "source": [
    "17. Explain the concept of affinity propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e20beb",
   "metadata": {},
   "source": [
    "Unlike K-means, Affinity Propagation does not require pre-defining the number of clusters.\n",
    "\n",
    "    Key idea:\n",
    "\n",
    "Each point sends and receives messages to decide which point is best suited as a cluster center (exemplar).\n",
    "\n",
    "Based on similarity matrix and message passing between points.\n",
    "\n",
    "Automatically finds the number of clusters based on a \"preference\" value.\n",
    "\n",
    "    Pros:\n",
    "\n",
    "Finds exemplars\n",
    "\n",
    "Works well for non-spherical clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c4d21",
   "metadata": {},
   "source": [
    "18. How do you handle categorical variables in clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9e867f",
   "metadata": {},
   "source": [
    "        Options to handle categorical data:\n",
    "\n",
    "One-Hot Encoding: Converts categorical values to binary vectors (works with algorithms like K-means but increases dimensionality).\n",
    "\n",
    "Frequency/Label Encoding: For ordinal data.\n",
    "\n",
    "    Distance-based methods:\n",
    "\n",
    "Hamming distance for binary/categorical data\n",
    "\n",
    "Gower distance: Handles mixed numerical and categorical types\n",
    "\n",
    "    Use clustering algorithms designed for categorical data like:\n",
    "\n",
    "K-Modes\n",
    "\n",
    "K-Prototypes (mixed data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74903d06",
   "metadata": {},
   "source": [
    "19. Describe the elbow method for determining the optimal number of clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0735b87",
   "metadata": {},
   "source": [
    "Goal: Find the best number of clusters (k) for K-Means.\n",
    "\n",
    "    Steps:\n",
    "\n",
    "1. Run K-Means for a range of k values (e.g., 1–10).\n",
    "\n",
    "2. Plot k vs Inertia (sum of squared distances to cluster centers).\n",
    "\n",
    "3. Look for the \"elbow\" point where the inertia stops decreasing significantly — that's the ideal k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c142d3",
   "metadata": {},
   "source": [
    "20. What are some emerging trends in clustering research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e41cf4",
   "metadata": {},
   "source": [
    "Deep Clustering: Combines neural networks with clustering (e.g., autoencoders + K-means).\n",
    "\n",
    "Self-Supervised Clustering: Learning representations + clusters without labels.\n",
    "\n",
    "Graph-based Clustering: Using Graph Neural Networks (GNNs) and community detection.\n",
    "\n",
    "Clustering on streaming data: Online or real-time clustering (e.g., CluStream).\n",
    "\n",
    "Explainable Clustering: Making cluster results interpretable and transparent.\n",
    "\n",
    "Clustering for LLMs and embeddings: E.g., clustering dense text/image embeddings from transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea78d93",
   "metadata": {},
   "source": [
    "21. What is anomaly detection, and why is it important \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c27d60",
   "metadata": {},
   "source": [
    "Anomaly detection is the task of identifying rare items, events, or patterns that differ significantly from the majority of the data.\n",
    "\n",
    "    Why important?\n",
    "\n",
    "Detect fraud (e.g., credit card fraud)\n",
    "\n",
    "Identify failures in systems (e.g., machinery, networks)\n",
    "\n",
    "Monitor cybersecurity threats (e.g., intrusions, phishing)\n",
    "\n",
    "Ensure data quality by detecting corrupt records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5944b",
   "metadata": {},
   "source": [
    "22. Discuss the types of anomalies encountered in anomaly detection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f3687",
   "metadata": {},
   "source": [
    "1. Point anomalies: A single instance is anomalous (e.g., very high transaction).\n",
    "\n",
    "2. Contextual anomalies: An anomaly only under specific context (e.g., a high temperature in winter).\n",
    "\n",
    "3. Collective anomalies: A group of data points are anomalous together (e.g., a sudden spike in network traffic).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe9f74",
   "metadata": {},
   "source": [
    "23. Explain the difference between supervised and unsupervised anomaly detection techniques \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1115c5d",
   "metadata": {},
   "source": [
    "| Feature          | Supervised                | Unsupervised                        |\n",
    "| ---------------- | ------------------------- | ----------------------------------- |\n",
    "| Labels available | Yes (normal vs. anomaly)  | No                                  |\n",
    "| Accuracy         | Higher (if enough labels) | Varies (no ground truth)            |\n",
    "| Algorithms used  | Classification models     | Clustering, density, distance-based |\n",
    "| Common examples  | Logistic regression, SVM  | Isolation Forest, LOF, DBSCAN       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee96102",
   "metadata": {},
   "source": [
    "24. Describe the Isolation Forest algorithm for anomaly detection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fbf19a",
   "metadata": {},
   "source": [
    "Isolation Forest works on the idea that anomalies are easier to isolate from the rest of the data.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "Builds random decision trees by splitting data.\n",
    "\n",
    "Shorter paths to isolate a point → likely an anomaly.\n",
    "\n",
    "Aggregates isolation depths to calculate an anomaly score.\n",
    "\n",
    "    Efficient for high-dimensional and large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f407244",
   "metadata": {},
   "source": [
    "25. How does One-Class SVM work in anomaly detection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664fb1a6",
   "metadata": {},
   "source": [
    "One-Class SVM tries to learn the boundary of the normal class.\n",
    "\n",
    "It fits a hypersphere (or hyperplane) around the normal data points.\n",
    "\n",
    "New data points falling outside this region are considered anomalies.\n",
    "\n",
    "    Good for:\n",
    "\n",
    "High-dimensional data\n",
    "\n",
    "When only normal data is available for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141829f8",
   "metadata": {},
   "source": [
    "26. Discuss the challenges of anomaly detection in high-dimensional data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4fef75",
   "metadata": {},
   "source": [
    "Curse of dimensionality: Distances become less meaningful.\n",
    "\n",
    "Feature redundancy or noise can mask anomalies.\n",
    "\n",
    "Scalability: Algorithms may be computationally expensive.\n",
    "\n",
    "Sparse anomalies: Few anomalous instances can make detection hard.\n",
    "\n",
    "Visualization and interpretability are limited.\n",
    "\n",
    "    Solutions: Use PCA, autoencoders, or feature selection to reduce dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9823a",
   "metadata": {},
   "source": [
    "27. Explain the concept of novelty detection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a2717",
   "metadata": {},
   "source": [
    "Novelty detection focuses on detecting new patterns that differ from known data during prediction.\n",
    "\n",
    "Trained only on normal data, and tested with unknown inputs.\n",
    "\n",
    "Useful when anomalies are not present in the training set.\n",
    "\n",
    "    Example: A model trained on healthy machine readings detects faults it has never seen before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a799441",
   "metadata": {},
   "source": [
    "28. What are some real-world applications of anomaly detection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614840f",
   "metadata": {},
   "source": [
    "Finance: Credit card fraud detection\n",
    "\n",
    "Healthcare: Disease outbreak or unusual health metrics\n",
    "\n",
    "Network security: Intrusion and malware detection\n",
    "\n",
    "Manufacturing: Predictive maintenance\n",
    "\n",
    "E-commerce: Fake reviews or pricing errors\n",
    "\n",
    "Banking: Money laundering and suspicious transactions\n",
    "\n",
    "IoT systems: Detecting faulty sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c55ea1",
   "metadata": {},
   "source": [
    "29. Describe the Local Outlier Factor (LOF) algorithm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b0ec9",
   "metadata": {},
   "source": [
    "LOF detects anomalies based on local density.\n",
    "\n",
    "    Concept:\n",
    "\n",
    "Compares the density of a point to its neighbors.\n",
    "\n",
    "If a point is in a low-density region compared to neighbors, it’s an outlier.\n",
    "\n",
    "    Key ideas:\n",
    "\n",
    "k-nearest neighbors (k-NN) used\n",
    "\n",
    "Score > 1 indicates an anomaly\n",
    "\n",
    "Works well in datasets with varying density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992f065",
   "metadata": {},
   "source": [
    "30. How do you evaluate the performance of an anomaly detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff200fbe",
   "metadata": {},
   "source": [
    "    If labels are available:\n",
    "\n",
    "Precision, Recall, F1-score (important for imbalanced data)\n",
    "\n",
    "ROC-AUC / PR-AUC (especially when anomalies are rare)\n",
    "\n",
    "Confusion matrix (TP, FP, FN, TN)\n",
    "\n",
    "    If labels are unavailable:\n",
    "\n",
    "Use domain knowledge\n",
    "\n",
    "Visualizations (e.g., t-SNE + outlier highlighting)\n",
    "\n",
    "Silhouette score or reconstruction error (e.g., autoencoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4bbeb",
   "metadata": {},
   "source": [
    "31. Discuss the role of feature engineering in anomaly detection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836615d",
   "metadata": {},
   "source": [
    "    Feature engineering plays a crucial role because:\n",
    "\n",
    "Relevant features improve anomaly detection accuracy.\n",
    "\n",
    "Helps highlight patterns and separate normal from anomalous behavior.\n",
    "\n",
    "Reduces noise and redundancy, especially in high-dimensional data.\n",
    "\n",
    "Enables temporal, domain-specific, or derived features (e.g., time since last transaction, average value, etc.).\n",
    "\n",
    "    Examples:\n",
    "\n",
    "In network intrusion detection: packet size, frequency, time gaps.\n",
    "\n",
    "In credit card fraud: time since last transaction, amount deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dbe108",
   "metadata": {},
   "source": [
    "32. What are the limitations of traditional anomaly detection methods \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0769416",
   "metadata": {},
   "source": [
    "Assume fixed data distribution (e.g., Gaussian) — not always true.\n",
    "\n",
    "Poor performance on high-dimensional or noisy data.\n",
    "\n",
    "Cannot handle concept drift (i.e., when normal behavior changes over time).\n",
    "\n",
    "Many require manual thresholds or tuning.\n",
    "\n",
    "Not robust against imbalanced datasets or unknown anomalies.\n",
    "\n",
    "May not adapt well to real-time streaming scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a0cb37",
   "metadata": {},
   "source": [
    "33. Explain the concept of ensemble methods in anomaly detection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efddd37",
   "metadata": {},
   "source": [
    "Ensemble methods combine multiple models to improve anomaly detection accuracy and robustness.\n",
    "\n",
    "    Types:\n",
    "\n",
    "Bagging: Combines multiple detectors trained on different random subsamples (e.g., Isolation Forest).\n",
    "\n",
    "Boosting: Sequentially trains models to correct previous errors.\n",
    "\n",
    "Hybrid: Combines statistical + ML models (e.g., LOF + autoencoder).\n",
    "\n",
    "Voting/Averaging: Aggregates anomaly scores from multiple models.\n",
    "\n",
    "    Benefits:\n",
    "\n",
    "Reduces variance\n",
    "\n",
    "Increases robustness\n",
    "\n",
    "Handles diverse types of anomalies better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e1fdd",
   "metadata": {},
   "source": [
    "34. How does autoencoder-based anomaly detection work \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f568c7e",
   "metadata": {},
   "source": [
    "Autoencoders are neural networks trained to reconstruct input data.\n",
    "\n",
    "    Process:\n",
    "\n",
    "1. Train autoencoder on normal data only.\n",
    "\n",
    "2. During inference:\n",
    "\n",
    "    Normal data → low reconstruction error.\n",
    "\n",
    "    Anomalies → high reconstruction error (as model has never seen them).\n",
    "\n",
    "3. Set a threshold on reconstruction loss to detect anomalies.\n",
    "\n",
    "        Useful for high-dimensional, complex datasets (e.g., images, time series)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16fd874",
   "metadata": {},
   "source": [
    "35. What are some approaches for handling imbalanced data in anomaly detection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41de44",
   "metadata": {},
   "source": [
    "Resampling:\n",
    "\n",
    "    Oversampling anomalies (e.g., SMOTE)\n",
    "\n",
    "    Undersampling normal data\n",
    "\n",
    "Anomaly score threshold tuning: Adjust to favor recall or precision\n",
    "\n",
    "Cost-sensitive learning: Penalize false negatives more\n",
    "\n",
    "Synthetic data generation: Create artificial anomalies\n",
    "\n",
    "Use anomaly-specific evaluation metrics: e.g., AUC-PR, F1 for rare classes\n",
    "\n",
    "Ensemble methods: Balance models to handle minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e259d61",
   "metadata": {},
   "source": [
    "36. Describe the concept of semi-supervised anomaly detection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e0ce8",
   "metadata": {},
   "source": [
    "Assumes only normal class is labeled during training.\n",
    "\n",
    "Learns patterns of normal behavior; anything different is an anomaly.\n",
    "\n",
    "More realistic in many applications (e.g., cybersecurity, healthcare).\n",
    "\n",
    "Models: One-Class SVM, autoencoders, semi-supervised deep learning.\n",
    "\n",
    "    Useful when:\n",
    "\n",
    "Labeled anomalies are rare\n",
    "\n",
    "Manual labeling is expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886566e",
   "metadata": {},
   "source": [
    "37. Discuss the trade-offs between false positives and false negatives in anomaly detection \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae63c7",
   "metadata": {},
   "source": [
    "    False Positive (FP): Normal instance marked as anomaly\n",
    "\n",
    "Can lead to alert fatigue, wasted resources\n",
    "\n",
    "    False Negative (FN): Anomaly missed as normal\n",
    "\n",
    "Can be critical in applications like fraud or disease detection\n",
    "\n",
    "    Trade-off depends on domain:\n",
    "\n",
    "In fraud detection → FN is worse\n",
    "\n",
    "In system alerts → Too many FP reduces trust\n",
    "\n",
    "    Balance using:\n",
    "\n",
    "Precision-Recall tuning\n",
    "\n",
    "Cost-based loss functions\n",
    "\n",
    "ROC/PR curve analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f1235a",
   "metadata": {},
   "source": [
    "38. How do you interpret the results of an anomaly detection model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570dabd",
   "metadata": {},
   "source": [
    "    Interpretation involves:\n",
    "\n",
    "Visualizing outliers using t-SNE, PCA, or UMAP\n",
    "\n",
    "Ranking by anomaly scores (e.g., top 5% most anomalous)\n",
    "\n",
    "Comparing results with domain knowledge\n",
    "\n",
    "Understanding model output (e.g., reconstruction error in autoencoder)\n",
    "\n",
    "Using feature attribution tools (e.g., SHAP, LIME) for black-box models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e85a9",
   "metadata": {},
   "source": [
    "39. What are some open research challenges in anomaly detection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aca72d",
   "metadata": {},
   "source": [
    "Label scarcity: Real anomalies are rare and hard to label\n",
    "\n",
    "Concept drift: Changes in normal behavior over time\n",
    "\n",
    "Explainability: Why a point is anomalous\n",
    "\n",
    "Dynamic & streaming data: Real-time detection is complex\n",
    "\n",
    "Multi-modal data: Text, image, audio combinations\n",
    "\n",
    "Data privacy and federated anomaly detection\n",
    "\n",
    "Adversarial robustness: Attackers may mask anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2463e4f8",
   "metadata": {},
   "source": [
    "40. Explain the concept of contextual anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd593a",
   "metadata": {},
   "source": [
    "Contextual anomalies are data points that are only anomalous within a specific context.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "High heart rate during exercise is normal, but not while sleeping.\n",
    "\n",
    "30°C temperature is normal in summer, but anomalous in winter.\n",
    "\n",
    "    Key components:\n",
    "\n",
    "Contextual attributes: Time, location, etc.\n",
    "\n",
    "Behavioral attributes: Values being analyzed\n",
    "\n",
    "    Models must consider context + behavior simultaneously. Often solved with:\n",
    "\n",
    "LSTM (for time series)\n",
    "\n",
    "Context-aware autoencoders\n",
    "\n",
    "Rule-based systems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5501e903",
   "metadata": {},
   "source": [
    "41. What is time series analysis, and what are its key components \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8871d2",
   "metadata": {},
   "source": [
    "Time series analysis involves methods to analyze data points collected or recorded over time (e.g., stock prices, weather, sales).\n",
    "\n",
    "    Key components:\n",
    "\n",
    "Trend: Long-term direction (upward/downward).\n",
    "\n",
    "Seasonality: Repeating patterns (e.g., daily, weekly, yearly).\n",
    "\n",
    "Cyclic behavior: Non-fixed, longer-term fluctuations.\n",
    "\n",
    "Noise (or residual): Random variations not explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee1834",
   "metadata": {},
   "source": [
    "42. Discuss the difference between univariate and multivariate time series analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f22c64",
   "metadata": {},
   "source": [
    "| Aspect          | Univariate                       | Multivariate                           |\n",
    "| --------------- | -------------------------------- | -------------------------------------- |\n",
    "| Input variables | Only one time-dependent variable | Two or more time-dependent variables   |\n",
    "| Example         | Temperature over time            | Temperature + humidity over time       |\n",
    "| Complexity      | Simpler models (ARIMA, ETS)      | More complex (VAR, LSTM multivariate)  |\n",
    "| Use cases       | Forecasting a single metric      | Capturing inter-variable relationships |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f291c82",
   "metadata": {},
   "source": [
    "43. Describe the process of time series decomposition \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06fbba8",
   "metadata": {},
   "source": [
    "Time series decomposition breaks down the original series into separate interpretable components:\n",
    "\n",
    "1. Trend extraction (e.g., via moving average)\n",
    "\n",
    "2. Seasonality detection\n",
    "\n",
    "3. Residual analysis (random error/noise)\n",
    "\n",
    "        Helps in understanding the data and building better forecasting models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c6196",
   "metadata": {},
   "source": [
    "44. What are the main components of a time series decomposition \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f55c3b",
   "metadata": {},
   "source": [
    "1. Trend (T): Long-term increase or decrease.\n",
    "\n",
    "2. Seasonality (S): Repeating short-term cycles.\n",
    "\n",
    "3. Residual (R): Irregular, random noise.\n",
    "\n",
    "4. Cycle (optional): Long-term fluctuations that aren’t seasonal.\n",
    "\n",
    "        Two models:\n",
    "\n",
    "Additive: Y(t)=T(t)+S(t)+R(t)\n",
    "\n",
    "Multiplicative: Y(t)=T(t)×S(t)×R(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30245158",
   "metadata": {},
   "source": [
    "45. Explain the concept of stationarity in time series data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f323195",
   "metadata": {},
   "source": [
    "    A time series is stationary if its:\n",
    "\n",
    "Mean, variance, and covariance are constant over time.\n",
    "\n",
    "    Why important?\n",
    "\n",
    "Most forecasting models (like ARIMA) require stationarity for validity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92484a24",
   "metadata": {},
   "source": [
    "46. How do you test for stationarity in a time series \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e07a713",
   "metadata": {},
   "source": [
    "1. Visual inspection: Plot to check for trends or changing variance.\n",
    "\n",
    "2. Rolling statistics: Mean & variance over sliding windows.\n",
    "\n",
    "3. Statistical tests:\n",
    "\n",
    "    ADF (Augmented Dickey-Fuller): Null hypothesis = non-stationary\n",
    "\n",
    "    KPSS (Kwiatkowski-Phillips-Schmidt-Shin): Null = stationary\n",
    "\n",
    "If not stationary:\n",
    "\n",
    "Apply differencing, log transforms, or seasonal adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f1b30",
   "metadata": {},
   "source": [
    "47. Discuss the autoregressive integrated moving average (ARIMA) model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1331208",
   "metadata": {},
   "source": [
    "    ARIMA is a forecasting model combining:\n",
    "\n",
    "AR (Autoregression): Uses past values\n",
    "\n",
    "I (Integrated): Differencing to make data stationary\n",
    "\n",
    "MA (Moving Average): Uses past forecast errors\n",
    "\n",
    "    Used for non-seasonal time series prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34698ec5",
   "metadata": {},
   "source": [
    "48. What are the parameters of the ARIMA model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c252143",
   "metadata": {},
   "source": [
    "    ARIMA is defined by three parameters: ARIMA(p, d, q)\n",
    "\n",
    "p: Order of AR (lag of past values)\n",
    "\n",
    "d: Order of differencing (to make data stationary)\n",
    "\n",
    "q: Order of MA (lag of past forecast errors)\n",
    "\n",
    "    Example:\n",
    "ARIMA(1,1,2) → 1 AR term, 1 differencing, 2 MA terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee0923",
   "metadata": {},
   "source": [
    "\n",
    "49. Describe the seasonal autoregressive integrated moving average (SARIMA) model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef1bdf1",
   "metadata": {},
   "source": [
    "    SARIMA extends ARIMA by adding seasonality:\n",
    "\n",
    "SARIMA(p, d, q)(P, D, Q, s)\n",
    "\n",
    "p, d, q: ARIMA terms\n",
    "\n",
    "P, D, Q: Seasonal components\n",
    "\n",
    "s: Season length (e.g., 12 for monthly data with yearly seasonality)\n",
    "\n",
    "    Captures both short-term and seasonal patterns in time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb07a93b",
   "metadata": {},
   "source": [
    "50. How do you choose the appropriate lag order in an ARIMA mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081aafc",
   "metadata": {},
   "source": [
    "1. ACF (Autocorrelation Function): Helps select q (MA terms)\n",
    "\n",
    "2. PACF (Partial Autocorrelation Function): Helps select p (AR terms)\n",
    "\n",
    "3. Differencing (d): Use until series is stationary\n",
    "\n",
    "4. Grid search or auto_arima (pmdarima): Automatically finds best (p,d,q)\n",
    "\n",
    "5. Information criteria:\n",
    "\n",
    "    AIC (Akaike Information Criterion)\n",
    "\n",
    "    BIC (Bayesian Information Criterion)\n",
    "\n",
    "Lower AIC/BIC = better model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02096d4",
   "metadata": {},
   "source": [
    "51. Explain the concept of differencing in time series analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42545103",
   "metadata": {},
   "source": [
    "    Differencing is a technique used to make a time series stationary by removing trends or seasonality.\n",
    "\n",
    "First-order differencing: yt′=yt−yt−1\n",
    "\n",
    "Second-order differencing: Apply differencing again on the result.\n",
    "\n",
    "    It helps in stabilizing the mean of the series, which is essential for models like ARIMA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39cf8bb",
   "metadata": {},
   "source": [
    "52. What is the Box-Jenkins methodology\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21bd6",
   "metadata": {},
   "source": [
    "The Box-Jenkins methodology is a 4-step iterative approach to time series modeling using ARIMA/SARIMA models.\n",
    "\n",
    "    Steps:\n",
    "\n",
    "1. Identification: Make data stationary; identify p, d, q using ACF/PACF.\n",
    "\n",
    "2. Estimation: Estimate model parameters (using MLE or least squares).\n",
    "\n",
    "3. Diagnostic checking: Examine residuals for white noise.\n",
    "\n",
    "4. Forecasting: Use the fitted model to predict future values.\n",
    "\n",
    "        This is the foundation for ARIMA-based forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86c66b",
   "metadata": {},
   "source": [
    "53. Discuss the role of ACF and PACF plots in identifying ARIMA parameters \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98258a",
   "metadata": {},
   "source": [
    "        ACF (Autocorrelation Function):\n",
    "\n",
    "Measures correlation between a time series and its lags.\n",
    "\n",
    "Helps determine q (MA order).\n",
    "\n",
    "        PACF (Partial Autocorrelation Function):\n",
    "\n",
    "Removes intermediate correlations.\n",
    "\n",
    "Helps determine p (AR order).\n",
    "\n",
    "    Pattern-based decisions:\n",
    "\n",
    "AR(p): Sharp cutoff in PACF\n",
    "\n",
    "MA(q): Sharp cutoff in ACF\n",
    "\n",
    "ARMA(p,q): Gradual decay in both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd51e39d",
   "metadata": {},
   "source": [
    "\n",
    "54. How do you handle missing values in time series data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9784e",
   "metadata": {},
   "source": [
    "    Methods:\n",
    "\n",
    "Forward fill / Backward fill (ffill/bfill)\n",
    "\n",
    "Linear interpolation\n",
    "\n",
    "Moving average smoothing\n",
    "\n",
    "Seasonal decomposition and imputation\n",
    "\n",
    "Model-based imputation (e.g., Kalman filters, ARIMA)\n",
    "\n",
    "Delete if very few missing points and not critical\n",
    "\n",
    "    Important to maintain temporal consistency when filling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3bf0eb",
   "metadata": {},
   "source": [
    "55. Describe the concept of exponential smoothing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb08b98d",
   "metadata": {},
   "source": [
    "Exponential smoothing assigns exponentially decreasing weights to past observations.\n",
    "\n",
    "    Formula:\n",
    "\n",
    "^yt=αyt−1+(1−α)^yt−1\n",
    "\n",
    "α ∈ [0, 1]: smoothing factor\n",
    "\n",
    "    Types:\n",
    "\n",
    "Simple: For data without trend/seasonality\n",
    "\n",
    "Double: Accounts for trend\n",
    "\n",
    "Triple (Holt-Winters): Handles trend + seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23090f8f",
   "metadata": {},
   "source": [
    "56. What is the Holt-Winters method, and when is it used?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83733f7",
   "metadata": {},
   "source": [
    "Holt-Winters is an extension of exponential smoothing for seasonal time series.\n",
    "\n",
    "    Types:\n",
    "\n",
    "Additive: For constant seasonal variation\n",
    "\n",
    "Multiplicative: For seasonality that scales with level\n",
    "\n",
    "    Components:\n",
    "\n",
    "Level (L)\n",
    "\n",
    "Trend (T)\n",
    "\n",
    "Seasonality (S)\n",
    "\n",
    "    Used when data exhibits both trend and seasonal patterns (e.g., retail sales, energy usage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7d60a",
   "metadata": {},
   "source": [
    "57. Discuss the challenges of forecasting long-term trends in time series data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e98a1",
   "metadata": {},
   "source": [
    "Unpredictable changes: Market shifts, policy changes, natural events\n",
    "\n",
    "Accumulated error: Forecast error grows over time\n",
    "\n",
    "Model overfitting: Too complex models may perform poorly long-term\n",
    "\n",
    "Concept drift: Underlying pattern may change\n",
    "\n",
    "Seasonal shifts: Future seasons may not match past patterns\n",
    "\n",
    "    Mitigation: Use ensemble models, confidence intervals, update models regularly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640acd3",
   "metadata": {},
   "source": [
    "58. Explain the concept of seasonality in time series analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a74c87",
   "metadata": {},
   "source": [
    "Seasonality is a repeating pattern or cycle in data at regular intervals.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "Daily: Website traffic peaks at certain hours\n",
    "\n",
    "Weekly: Shopping habits on weekends\n",
    "\n",
    "Yearly: Weather or sales patterns\n",
    "\n",
    "    Handled using:\n",
    "\n",
    "Seasonal decomposition\n",
    "\n",
    "SARIMA\n",
    "\n",
    "Holt-Winters\n",
    "\n",
    "Fourier terms in ML models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e966ac",
   "metadata": {},
   "source": [
    "\n",
    "59. How do you evaluate the performance of a time series forecasting model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e403de57",
   "metadata": {},
   "source": [
    "    Common metrics:\n",
    "\n",
    "MAE (Mean Absolute Error)\n",
    "\n",
    "RMSE (Root Mean Squared Error)\n",
    "\n",
    "MAPE (Mean Absolute Percentage Error) – beware of division by zero\n",
    "\n",
    "SMAPE (Symmetric MAPE)\n",
    "\n",
    "R² Score\n",
    "\n",
    "Cross-validation: Time series split (not random split!)\n",
    "\n",
    "    Visual tools:\n",
    "\n",
    "Forecast vs. actual plot\n",
    "\n",
    "Residual plot to detect bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3122d",
   "metadata": {},
   "source": [
    "60. What are some advanced techniques for time series forecasting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3db261",
   "metadata": {},
   "source": [
    "Facebook Prophet: Robust, interpretable, handles holidays and missing data\n",
    "\n",
    "LSTM/GRU (Recurrent Neural Networks): Learn long-term dependencies\n",
    "\n",
    "Temporal Convolutional Networks (TCN)\n",
    "\n",
    "Transformer models: For multivariate or high-dimensional forecasting (e.g., Informer, TimeGPT)\n",
    "\n",
    "AutoML models: AutoTS, H2O-AutoML, GluonTS\n",
    "\n",
    "Hybrid models: Combine statistical + deep learning (e.g., ARIMA + LSTM)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
