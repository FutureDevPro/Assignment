{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623cf6d5",
   "metadata": {},
   "source": [
    "1. What is regression analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a284c7",
   "metadata": {},
   "source": [
    "Regression analysis is a statistical technique used to model and analyze the relationship between a dependent variable (target) and one or more independent variables (features).\n",
    "\n",
    "    Purpose:\n",
    "\n",
    "Predict numeric outcomes\n",
    "\n",
    "Understand the influence of input variables on the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab64155",
   "metadata": {},
   "source": [
    "2. Explain the difference between linear and nonlinear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc42d0",
   "metadata": {},
   "source": [
    "| Aspect           | Linear Regression                 | Nonlinear Regression                                   |\n",
    "| ---------------- | --------------------------------- | ------------------------------------------------------ |\n",
    "| Relationship     | Assumes a **linear relationship** | Assumes a **nonlinear relationship**                   |\n",
    "| Equation         | $y = b_0 + b_1x$                  | $y = b_0 + b_1x + b_2x^2 + \\dots$ or exponential, etc. |\n",
    "| Interpretability | Easier to interpret               | Often complex to interpret                             |\n",
    "| Computation      | Simpler and faster                | More computationally intensive                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c4380",
   "metadata": {},
   "source": [
    "3. What is the difference between simple linear regression and multiple linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f69127",
   "metadata": {},
   "source": [
    "| Feature            | Simple Linear Regression | Multiple Linear Regression                   |\n",
    "| ------------------ | ------------------------ | -------------------------------------------- |\n",
    "| Number of features | One independent variable | Two or more independent variables            |\n",
    "| Equation           | $y = b_0 + b_1x$         | $y = b_0 + b_1x_1 + b_2x_2 + \\dots + b_nx_n$ |\n",
    "| Complexity         | Lower                    | Higher                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8dbb8",
   "metadata": {},
   "source": [
    "4. How is the performance of a regression model typically evaluated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd9c270",
   "metadata": {},
   "source": [
    "    Metrics used:\n",
    "\n",
    "Mean Absolute Error (MAE): Average of absolute differences between predicted and actual values.\n",
    "\n",
    "Mean Squared Error (MSE): Average of squared differences.\n",
    "\n",
    "Root Mean Squared Error (RMSE): Square root of MSE.\n",
    "\n",
    "R-squared (R²): Proportion of variance in the target explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc273a5",
   "metadata": {},
   "source": [
    "5. What is overfitting in the context of regression models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea926e3",
   "metadata": {},
   "source": [
    "Overfitting occurs when a model learns the training data too well, including noise and outliers, and fails to generalize to new/unseen data.\n",
    "\n",
    "    Symptoms:\n",
    "\n",
    "High accuracy on training data\n",
    "\n",
    "Poor performance on validation/test data\n",
    "\n",
    "    Common causes:\n",
    "\n",
    "Too many features\n",
    "\n",
    "Complex model\n",
    "\n",
    "Small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844de34",
   "metadata": {},
   "source": [
    "6. What is logistic regression used for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b527a",
   "metadata": {},
   "source": [
    "Logistic Regression is used for binary or multi-class classification, not regression.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "Spam detection (Spam vs. Not Spam)\n",
    "\n",
    "Disease prediction (Yes/No)\n",
    "\n",
    "Credit default (Defaulted/Not Defaulted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e59561",
   "metadata": {},
   "source": [
    "7. How does logistic regression differ from linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e666f7",
   "metadata": {},
   "source": [
    "| Feature         | Linear Regression               | Logistic Regression                 |\n",
    "| --------------- | ------------------------------- | ----------------------------------- |\n",
    "| Output          | Continuous values               | Probability between 0 and 1         |\n",
    "| Use Case        | Regression (predicting numbers) | Classification (predicting classes) |\n",
    "| Output Function | Linear equation                 | **Sigmoid function**                |\n",
    "| Target Variable | Real number                     | Categorical (e.g., 0 or 1)          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81e7ee",
   "metadata": {},
   "source": [
    "8. Explain the concept of odds ratio in logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4858b39",
   "metadata": {},
   "source": [
    "The odds ratio compares the odds of an event happening to it not happening.\n",
    "\n",
    "Odds= P(event)/1−P(event)\n",
    "\n",
    "    ​Odds Ratio:\n",
    "\n",
    "1: Positive association\n",
    "\n",
    "<1: Negative association\n",
    "\n",
    "=1: No effect\n",
    "\n",
    "In logistic regression, the exponentiated coefficients (e^β) represent odds ratios for each predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e923b333",
   "metadata": {},
   "source": [
    "9. What is the sigmoid function in logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64812e23",
   "metadata": {},
   "source": [
    "The sigmoid function maps any real-valued number to a value between 0 and 1, representing probability.\n",
    "\n",
    "σ(z)= 1/1+e^−z\n",
    "\n",
    "Where \n",
    "z=b0+b1x1+b2x2+…\n",
    "\n",
    "Used in logistic regression to convert linear output to a probability of class membership."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07bd858",
   "metadata": {},
   "source": [
    "10. How is the performance of a logistic regression model evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a4e77",
   "metadata": {},
   "source": [
    "    Evaluation Metrics:\n",
    "\n",
    "Accuracy: % of correctly predicted labels\n",
    "\n",
    "Precision: Correct positive predictions / total predicted positives\n",
    "\n",
    "Recall (Sensitivity): Correct positive predictions / actual positives\n",
    "\n",
    "F1-score: Harmonic mean of precision and recall\n",
    "\n",
    "ROC-AUC Score: Measures classification performance across thresholds\n",
    "\n",
    "Confusion Matrix: Shows TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef74257",
   "metadata": {},
   "source": [
    "11. What is a decision tree \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90010fc7",
   "metadata": {},
   "source": [
    "A Decision Tree is a supervised learning algorithm used for classification and regression.\n",
    "It splits data into branches based on feature values, forming a tree-like structure where:\n",
    "\n",
    "Internal nodes → decision based on a feature\n",
    "\n",
    "Leaf nodes → prediction (class/label or value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89593787",
   "metadata": {},
   "source": [
    "12. How does a decision tree make predictions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682834b1",
   "metadata": {},
   "source": [
    "Prediction is made by traversing the tree from the root to a leaf:\n",
    "\n",
    "    At each node, the algorithm checks a feature condition (e.g., X < 5)\n",
    "\n",
    "    Based on the outcome, it moves left or right\n",
    "\n",
    "    When a leaf node is reached, the associated value (class/number) is returned as the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d126e2c",
   "metadata": {},
   "source": [
    "13. What is entropy in the context of decision trees \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6b528",
   "metadata": {},
   "source": [
    "Entropy measures the impurity or randomness in a dataset.\n",
    "         \n",
    "Entropy(S)=−n∑i=1 (pilog base2(pi))\n",
    "\n",
    "pi= proportion of class \n",
    "Lower entropy → purer data → better split\n",
    "\n",
    "Used in ID3 algorithm to decide the best feature for splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea782f5",
   "metadata": {},
   "source": [
    "14. What is pruning in decision trees \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c0c8c",
   "metadata": {},
   "source": [
    "Pruning reduces the size of a decision tree by removing branches that provide little or no value.\n",
    "\n",
    "    Types:\n",
    "\n",
    "Pre-pruning: Stop tree growth early based on conditions (e.g., max depth, min samples).\n",
    "\n",
    "Post-pruning: Grow the full tree and then remove nodes using validation performance.\n",
    "\n",
    "    Prevents overfitting and improves generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905885ac",
   "metadata": {},
   "source": [
    "15. How do decision trees handle missing values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424800e",
   "metadata": {},
   "source": [
    "    Decision trees can handle missing values by:\n",
    "\n",
    "Skipping missing features during split decisions\n",
    "\n",
    "Surrogate splits: Use another feature that closely mimics the original split\n",
    "\n",
    "Assigning probabilities: Split instance based on observed distributions of known values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a9289a",
   "metadata": {},
   "source": [
    "16. What is a support vector machine (SVM) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f1792",
   "metadata": {},
   "source": [
    "SVM is a supervised learning algorithm used for binary classification, multi-class classification, and regression.\n",
    "\n",
    "It finds the optimal hyperplane that best separates classes with the maximum margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0aeb1",
   "metadata": {},
   "source": [
    "17. Explain the concept of margin in SVM \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8facf849",
   "metadata": {},
   "source": [
    "    Margin is the distance between the hyperplane and the nearest data points from each class.\n",
    "\n",
    "Maximum Margin Classifier: Chooses the hyperplane that maximizes this margin\n",
    "\n",
    "Larger margin = better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02039af9",
   "metadata": {},
   "source": [
    "18. What are support vectors in SVM \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea3f6e1",
   "metadata": {},
   "source": [
    "Support Vectors are the data points closest to the separating hyperplane.\n",
    "\n",
    "They are critical in defining the position and orientation of the hyperplane.\n",
    "\n",
    "    Only support vectors affect the model; removing others won't change the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96e0bc",
   "metadata": {},
   "source": [
    "19. How does SVM handle non-linearly separable data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f76f75",
   "metadata": {},
   "source": [
    "    Two main techniques:\n",
    "\n",
    "1. Soft Margin SVM: Allows some misclassifications using a penalty parameter C.\n",
    "\n",
    "2. Kernel Trick:\n",
    "\n",
    "    Transforms data into higher dimensions using a kernel (e.g., RBF, polynomial).\n",
    "\n",
    "    Makes non-linear data linearly separable in transformed space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a77cb2",
   "metadata": {},
   "source": [
    "20. What are the advantages of SVM over other classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d88565",
   "metadata": {},
   "source": [
    "    Advantages of SVM:\n",
    "\n",
    "Works well in high-dimensional spaces\n",
    "\n",
    "Effective when number of features > number of samples\n",
    "\n",
    "Robust to overfitting (especially with proper regularization)\n",
    "\n",
    "Can handle non-linear data using kernel trick\n",
    "\n",
    "Uses only support vectors, making it memory-efficient\n",
    "\n",
    "    Limitations:\n",
    "\n",
    "Not ideal for large datasets (training is slow)\n",
    "\n",
    "Performance depends on choice of kernel and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a34b4",
   "metadata": {},
   "source": [
    "21. What is the Naïve Bayes algorithm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3160f66",
   "metadata": {},
   "source": [
    "Naïve Bayes is a supervised learning algorithm based on Bayes' Theorem, used primarily for classification tasks.\n",
    "\n",
    "It assumes that features are conditionally independent given the class label.\n",
    "\n",
    "    Bayes' Theorem:\n",
    "\n",
    "P(Y∣X)= P(X∣Y)⋅P(Y)/P(X)\n",
    "\n",
    "    ​Where:\n",
    "    Y = target class\n",
    "    X = features\n",
    "    P(Y∣X) = posterior\n",
    "    P(X∣Y) = likelihood\n",
    "    P(Y) = prior\n",
    "    P(X) = evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcefd60",
   "metadata": {},
   "source": [
    "22. Why is it called \"Naïve\" Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed955c",
   "metadata": {},
   "source": [
    "It’s called \"Naïve\" because it assumes all features are independent of each other given the class label.\n",
    "\n",
    "This assumption is rarely true in practice, hence \"naïve\", but it still performs surprisingly well for many tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e704e",
   "metadata": {},
   "source": [
    "23. How does Naïve Bayes handle continuous and categorical features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1587031",
   "metadata": {},
   "source": [
    "    Categorical features:\n",
    "\n",
    "Uses frequency-based probabilities (e.g., count of class-label matches)\n",
    "\n",
    "    Continuous features:\n",
    "\n",
    "Assumes a distribution (typically Gaussian/normal)\n",
    "\n",
    "Computes probabilities using the probability density function (PDF) of that distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b060c",
   "metadata": {},
   "source": [
    "24. Explain the concept of prior and posterior probabilities in Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb8f006",
   "metadata": {},
   "source": [
    "Prior (P(Y)): Probability of a class before seeing the data.\n",
    "\n",
    "    Example: If 70 out of 100 emails are spam = 0.7\n",
    "\n",
    "Posterior (P(Y∣X)): Updated probability of the class after seeing the data/features.\n",
    "\n",
    "    Calculated using Bayes’ Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c306f321",
   "metadata": {},
   "source": [
    "25. What is Laplace smoothing and why is it used in Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c69dd",
   "metadata": {},
   "source": [
    "Laplace smoothing (add-one smoothing) is used to handle zero probabilities when a category in a feature is not present in the training data for a class.\n",
    "\n",
    "    Formula (for categorical data):\n",
    "\n",
    "P(xi∣y) =  count(xi,y)+1/count(y)+k\n",
    "\n",
    "​Where:\n",
    "\n",
    "    k = number of categories in feature\n",
    "    Prevents multiplication by zero in likelihood calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2cd6c7",
   "metadata": {},
   "source": [
    "26. Can Naïve Bayes be used for regression tasks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355235f2",
   "metadata": {},
   "source": [
    "While Naïve Bayes is primarily used for classification, there is a variant called Naïve Bayes regression, but it's rarely used in practice.\n",
    "\n",
    "Other models like linear regression or decision tree regression are typically preferred for continuous outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1883e2b7",
   "metadata": {},
   "source": [
    "27. How do you handle missing values in Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e8991",
   "metadata": {},
   "source": [
    "    Ignoring missing features during probability calculation\n",
    "\n",
    "    Imputing missing values using:\n",
    "\n",
    "        Mean/median (for continuous features)\n",
    "\n",
    "        Mode (for categorical features)\n",
    "\n",
    "    Using probabilistic imputation based on class-specific statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed4028",
   "metadata": {},
   "source": [
    "28. What are some common applications of Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237ea4d",
   "metadata": {},
   "source": [
    "Text classification (spam detection, sentiment analysis)\n",
    "\n",
    "Email filtering\n",
    "\n",
    "Medical diagnosis\n",
    "\n",
    "Document categorization\n",
    "\n",
    "Recommendation systems\n",
    "\n",
    "    It works well with high-dimensional data, like text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1d4c7",
   "metadata": {},
   "source": [
    "29. Explain the concept of feature independence assumption in Naïve Bayes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de4173",
   "metadata": {},
   "source": [
    "The feature independence assumption means that Naïve Bayes assumes all features contribute independently to the probability of a class.\n",
    "\n",
    "P(x1,x2,...,xn∣y)=P(x1∣y)⋅P(x2∣y)⋅...⋅P(xn∣y)\n",
    "\n",
    "    Simplifies computation\n",
    "\n",
    "    Often violated in real-world data, but the model still performs well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf83e03",
   "metadata": {},
   "source": [
    "30. How does Naïve Bayes handle categorical features with a large number of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86bc38",
   "metadata": {},
   "source": [
    "    Challenges:\n",
    "\n",
    "Sparse data: Many categories may appear rarely\n",
    "\n",
    "Zero-frequency problem: More chances of categories not being seen in training data\n",
    "\n",
    "    Solutions:\n",
    "\n",
    "Laplace smoothing to avoid zero probabilities\n",
    "\n",
    "Category grouping (combine rare categories into \"Other\")\n",
    "\n",
    "Use feature hashing to reduce dimensionality\n",
    "\n",
    "Use embedding-based techniques (in advanced models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739db5cb",
   "metadata": {},
   "source": [
    "31. What is the curse of dimensionality, and how does it affect machine learning algorithms \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8619a42",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to the various problems that arise when working with high-dimensional data (many features).\n",
    "\n",
    "    Effects:\n",
    "\n",
    "Data becomes sparse, making patterns harder to detect.\n",
    "\n",
    "Distance metrics become less meaningful (important in KNN, clustering).\n",
    "\n",
    "Increases computation time and risk of overfitting.\n",
    "\n",
    "    Solution: Use dimensionality reduction (e.g., PCA, feature selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc712cb",
   "metadata": {},
   "source": [
    "32. Explain the bias-variance tradeoff and its implications for machine learning models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a70ae",
   "metadata": {},
   "source": [
    "| Component    | Description                                                               | Effect                    |\n",
    "| ------------ | ------------------------------------------------------------------------- | ------------------------- |\n",
    "| **Bias**     | Error due to overly **simple assumptions** in the model                   | Leads to **underfitting** |\n",
    "| **Variance** | Error due to model’s **sensitivity to fluctuations** in the training data | Leads to **overfitting**  |\n",
    "    Tradeoff:\n",
    "\n",
    "Low bias → high variance\n",
    "\n",
    "Low variance → high bias\n",
    "\n",
    "\n",
    "    Goal: Find the right balance for good generalization on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf993e8a",
   "metadata": {},
   "source": [
    "33. What is cross-validation, and why is it used \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418ad99",
   "metadata": {},
   "source": [
    "Cross-validation is a technique for evaluating model performance by dividing the dataset into training and validation sets multiple times.\n",
    "\n",
    "    Common type: K-Fold Cross-Validation\n",
    "\n",
    "Split data into k folds\n",
    "\n",
    "Train on (k-1) folds, test on 1 fold\n",
    "\n",
    "Repeat for all folds and average results\n",
    "\n",
    "    Prevents overfitting and ensures model generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42390b",
   "metadata": {},
   "source": [
    "34. Explain the difference between parametric and non-parametric machine learning algorithms \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36da099",
   "metadata": {},
   "source": [
    "| Aspect      | Parametric Algorithms                       | Non-Parametric Algorithms                |\n",
    "| ----------- | ------------------------------------------- | ---------------------------------------- |\n",
    "| Assumptions | Assumes a **fixed form** for the model      | No assumption about data distribution    |\n",
    "| Parameters  | **Finite** number of parameters             | Parameters grow with data                |\n",
    "| Examples    | Linear regression, Logistic regression, SVM | KNN, Decision Trees, Random Forest       |\n",
    "| Flexibility | Less flexible                               | More flexible, captures complex patterns |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0059d",
   "metadata": {},
   "source": [
    "35. What is feature scaling, and why is it important in machine learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779949e1",
   "metadata": {},
   "source": [
    "Feature scaling is the process of normalizing or standardizing features to bring them to a common scale.\n",
    "\n",
    "    Importance:\n",
    "\n",
    "Required for models that use distance (e.g., KNN, SVM, K-Means) or gradient descent.\n",
    "\n",
    "Prevents dominance of high-scale features.\n",
    "\n",
    "Helps models converge faster.\n",
    "\n",
    "    Common methods: Min-Max Scaling, Standardization, Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433f9b6",
   "metadata": {},
   "source": [
    "36. What is regularization, and why is it used in machine learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060dac51",
   "metadata": {},
   "source": [
    "Regularization adds a penalty to the loss function to discourage complex models, helping prevent overfitting.\n",
    "\n",
    "    Types:\n",
    "\n",
    "L1 Regularization (Lasso): Adds absolute values of coefficients → leads to feature selection\n",
    "\n",
    "L2 Regularization (Ridge): Adds squared values of coefficients → shrinks coefficients but keeps all features\n",
    "\n",
    "    Encourages simpler, more generalizable models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27794e82",
   "metadata": {},
   "source": [
    "37. Explain the concept of ensemble learning and give an example \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf6f75",
   "metadata": {},
   "source": [
    "Ensemble learning combines multiple models (weak learners) to produce better performance than individual models.\n",
    "\n",
    "    Improves accuracy, robustness, and stability\n",
    "\n",
    "    Example:\n",
    "\n",
    "Random Forest: Ensemble of decision trees using bagging\n",
    "\n",
    "Gradient Boosting: Ensemble of weak learners trained sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d99e1",
   "metadata": {},
   "source": [
    "38. What is the difference between bagging and boosting \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339403c2",
   "metadata": {},
   "source": [
    "| Feature        | Bagging                               | Boosting                             |\n",
    "| -------------- | ------------------------------------- | ------------------------------------ |\n",
    "| Model Training | Trains models **in parallel**         | Trains models **sequentially**       |\n",
    "| Goal           | Reduce **variance**                   | Reduce **bias and variance**         |\n",
    "| Example        | Random Forest                         | AdaBoost, XGBoost, Gradient Boosting |\n",
    "| Data Sampling  | Bootstrap sampling (with replacement) | Weighted sampling based on errors    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a87d3b4",
   "metadata": {},
   "source": [
    "39. What is the difference between a generative model and a discriminative model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d682f0",
   "metadata": {},
   "source": [
    "| Model Type         | Generative Model                | Discriminative Model                     |      |\n",
    "| ------------------ | ------------------------------- | ---------------------------------------- | ---- |\n",
    "| Learns             | **Joint probability** $P(X, Y)$ | **Conditional probability** ( P(Y        | X) ) |\n",
    "| Examples           | Naïve Bayes, GANs               | Logistic Regression, SVM, Decision Trees |      |\n",
    "| Goal               | Learn how data is generated     | Learn decision boundary between classes  |      |\n",
    "| Can generate data? | Yes                             | No                                       |      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac759b",
   "metadata": {},
   "source": [
    "40. Explain the concept of batch gradient descent and stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927cebc",
   "metadata": {},
   "source": [
    "    Batch Gradient Descent:\n",
    "\n",
    "Uses entire dataset to compute gradients\n",
    "\n",
    "More stable, but slower and memory-intensive\n",
    "\n",
    "    Stochastic Gradient Descent (SGD):\n",
    "\n",
    "Uses one data point at a time to update weights\n",
    "\n",
    "Faster, but has more variance in updates (noisy path to convergence)\n",
    "\n",
    "    Mini-batch Gradient Descent: A compromise → uses a small batch of data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4abf3",
   "metadata": {},
   "source": [
    "41. What is the K-nearest neighbors (KNN) algorithm, and how does it work \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a51751",
   "metadata": {},
   "source": [
    "KNN is a non-parametric, instance-based learning algorithm used for both classification and regression.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "Choose the number of neighbors k.\n",
    "\n",
    "Compute distance (e.g., Euclidean) between test point and all training points.\n",
    "\n",
    "Select the k-nearest points.\n",
    "\n",
    "For classification: use majority vote.\n",
    "\n",
    "For regression: use mean/median of neighbors’ values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550fa8c",
   "metadata": {},
   "source": [
    "42. What are the disadvantages of the K-nearest neighbors algorithm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb0415",
   "metadata": {},
   "source": [
    "Computationally expensive: Slow during prediction (no training phase)\n",
    "\n",
    "Sensitive to irrelevant features and feature scaling\n",
    "\n",
    "Memory intensive: Stores the entire dataset\n",
    "\n",
    "Performs poorly with high-dimensional data (curse of dimensionality)\n",
    "\n",
    "No model interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f415742",
   "metadata": {},
   "source": [
    "43. Explain the concept of one-hot encoding and its use in machine learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce2dfc",
   "metadata": {},
   "source": [
    "One-hot encoding converts categorical variables into binary columns, with one column per category.\n",
    "\n",
    "    Example:\n",
    "Color: Red, Blue, Green → [Red, Blue, Green]\n",
    "Red → [1, 0, 0]\n",
    "Blue → [0, 1, 0]\n",
    "\n",
    "    Why it’s used:\n",
    "\n",
    "Converts non-numeric data to numeric\n",
    "\n",
    "Prevents models from assuming ordinal relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad629aab",
   "metadata": {},
   "source": [
    "44. What is feature selection, and why is it important in machine learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a5c26",
   "metadata": {},
   "source": [
    "Feature selection is the process of identifying and using only the most relevant features for training a model.\n",
    "\n",
    "    Importance:\n",
    "\n",
    "Reduces overfitting\n",
    "\n",
    "Improves accuracy and training speed\n",
    "\n",
    "Makes models simpler and interpretable\n",
    "\n",
    "Helps in dealing with high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af1894",
   "metadata": {},
   "source": [
    "45. Explain the concept of cross-entropy loss and its use in classification tasks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529f9b9",
   "metadata": {},
   "source": [
    "Cross-entropy loss measures the difference between predicted probabilities and actual class labels.\n",
    "\n",
    "Formula (binary):\n",
    "\n",
    "L=−[y⋅log(p)+(1−y)⋅log(1−p)]\n",
    "\n",
    "    Where:\n",
    "\n",
    "y = true label (0 or 1)\n",
    "\n",
    "p = predicted probability of class 1\n",
    "\n",
    "    Commonly used in logistic regression, neural networks, and other classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a1ee0e",
   "metadata": {},
   "source": [
    "46. What is the difference between batch learning and online learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c086f9d",
   "metadata": {},
   "source": [
    "| Feature            | Batch Learning                            | Online Learning                             |\n",
    "| ------------------ | ----------------------------------------- | ------------------------------------------- |\n",
    "| Data Processing    | Uses **entire dataset** at once           | Uses **one sample or mini-batch** at a time |\n",
    "| Memory Requirement | High                                      | Low                                         |\n",
    "| Flexibility        | Static – retraining required for new data | Dynamic – updates incrementally             |\n",
    "| Use Case           | Stable datasets                           | Streaming data, real-time systems           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357db962",
   "metadata": {},
   "source": [
    "47. Explain the concept of grid search and its use in hyperparameter tuning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162cc374",
   "metadata": {},
   "source": [
    "Grid search is a brute-force method to find the best combination of hyperparameters by exhaustively trying all combinations in a predefined grid.\n",
    "\n",
    "    Steps:\n",
    "\n",
    "1. Define ranges/sets of hyperparameters.\n",
    "\n",
    "2. Train model on each combination using cross-validation.\n",
    "\n",
    "3. Select the one with best performance.\n",
    "\n",
    "\n",
    "        Guarantees optimal result (if search space is small)\n",
    "\n",
    "        Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3a592",
   "metadata": {},
   "source": [
    "48. What are the advantages and disadvantages of decision trees \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc4229",
   "metadata": {},
   "source": [
    "    Advantages:\n",
    "\n",
    "Easy to understand and visualize\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Handles both categorical and numerical data\n",
    "\n",
    "Non-parametric → flexible to data shapes\n",
    "\n",
    "    Disadvantages:\n",
    "\n",
    "Prone to overfitting (especially deep trees)\n",
    "\n",
    "Unstable to small changes in data\n",
    "\n",
    "Biased toward features with more levels\n",
    "\n",
    "Can be less accurate compared to ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ee78b",
   "metadata": {},
   "source": [
    "49. What is the difference between L1 and L2 regularization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f5bfb7",
   "metadata": {},
   "source": [
    "| Feature      | L1 Regularization (Lasso)           | L2 Regularization (Ridge)         |   |                      |\n",
    "| ------------ | ----------------------------------- | --------------------------------- | - | -------------------- |\n",
    "| Penalty term | ( \\lambda \\sum                      | w\\_i                              | ) | $\\lambda \\sum w_i^2$ |\n",
    "| Effect       | Can shrink some weights to **zero** | Shrinks all weights, none to zero |   |                      |\n",
    "| Use case     | **Feature selection**               | Handles multicollinearity well    |   |                      |\n",
    "\n",
    "    L1 → sparse models\n",
    "\n",
    "    L2 → smoother solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd093d8f",
   "metadata": {},
   "source": [
    "50. What are some common preprocessing techniques used in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd19cfc",
   "metadata": {},
   "source": [
    "1. Missing value handling: Imputation (mean, median, mode)\n",
    "\n",
    "2. Encoding categorical variables: One-hot, label, ordinal encoding\n",
    "\n",
    "3. Feature scaling: Min-Max, Standardization\n",
    "\n",
    "4. Outlier detection/removal\n",
    "\n",
    "5. Dimensionality reduction: PCA, t-SNE\n",
    "\n",
    "6. Text preprocessing: Tokenization, stemming, TF-IDF\n",
    "\n",
    "7. Data transformation: Log, sqrt, Box-Cox\n",
    "\n",
    "8. Balancing classes: SMOTE, undersampling, oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ef1e1",
   "metadata": {},
   "source": [
    " 51. What is the difference between a parametric and non-parametric algorithm? Give examples of each\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b3b64",
   "metadata": {},
   "source": [
    "| Feature      | Parametric Algorithms                               | Non-Parametric Algorithms                             |\n",
    "| ------------ | --------------------------------------------------- | ----------------------------------------------------- |\n",
    "| Assumption   | Fixed number of parameters                          | No fixed structure; grows with data                   |\n",
    "| Flexibility  | Less flexible, faster                               | More flexible, slower                                 |\n",
    "| Memory usage | Lower                                               | Higher                                                |\n",
    "| Examples     | Linear Regression, Logistic Regression, Naïve Bayes | KNN, Decision Trees, SVM (with kernel), Random Forest |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf08a2",
   "metadata": {},
   "source": [
    "52. Explain the bias-variance tradeoff and how it relates to model complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a576d7c",
   "metadata": {},
   "source": [
    "Bias: Error from simplistic models that can’t capture data complexity → leads to underfitting.\n",
    "\n",
    "Variance: Error from complex models that learn noise in training data → leads to overfitting.\n",
    "\n",
    "    Relation to complexity:\n",
    "\n",
    "Simple models → high bias, low variance\n",
    "\n",
    "Complex models → low bias, high variance\n",
    "\n",
    "    Goal: Find the sweet spot where total error (bias² + variance + noise) is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31439ac4",
   "metadata": {},
   "source": [
    "53. What are the advantages and disadvantages of using ensemble methods like random forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc3389",
   "metadata": {},
   "source": [
    "    Advantages:\n",
    "\n",
    "Higher accuracy and robustness than individual models\n",
    "\n",
    "Handles non-linear relationships well\n",
    "\n",
    "Reduces overfitting (compared to a single decision tree)\n",
    "\n",
    "Handles missing values and imbalanced data\n",
    "\n",
    "    Disadvantages:\n",
    "\n",
    "Less interpretable\n",
    "\n",
    "Requires more computation and memory\n",
    "\n",
    "Slower in training and prediction than a single tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7a704",
   "metadata": {},
   "source": [
    "54. Explain the difference between bagging and boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69618687",
   "metadata": {},
   "source": [
    "| Aspect         | Bagging                                      | Boosting                             |\n",
    "| -------------- | -------------------------------------------- | ------------------------------------ |\n",
    "| Model Training | Trains models **in parallel**                | Trains models **sequentially**       |\n",
    "| Focus          | Reduces **variance**                         | Reduces **bias and variance**        |\n",
    "| Data Sampling  | Bootstrap sampling (random with replacement) | Weighted sampling, focuses on errors |\n",
    "| Examples       | Random Forest                                | AdaBoost, XGBoost, Gradient Boosting |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10fcab",
   "metadata": {},
   "source": [
    "55. What is the purpose of hyperparameter tuning in machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc7f95",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is the process of optimizing the external configuration (not learned during training) to improve model performance.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "k in KNN\n",
    "\n",
    "C, gamma in SVM\n",
    "\n",
    "learning rate in gradient boosting\n",
    "\n",
    "    Helps achieve better accuracy, less overfitting, and faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c019f6",
   "metadata": {},
   "source": [
    "56. What is the difference between regularization and feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a1fa7",
   "metadata": {},
   "source": [
    "| Feature | Regularization                                | Feature Selection                        |\n",
    "| ------- | --------------------------------------------- | ---------------------------------------- |\n",
    "| Purpose | **Shrink coefficients** to reduce overfitting | **Select subset of features**            |\n",
    "| Method  | Penalty terms added to loss function          | Manual/statistical/model-based selection |\n",
    "| Example | L1/L2 regularization                          | RFE, Chi-square, feature importance      |\n",
    "| Outcome | Keeps all features but reduces impact         | Removes irrelevant/redundant features    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67f4961",
   "metadata": {},
   "source": [
    "57. How does the Lasso (L1) regularization differ from Ridge (L2) regularization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403faf7c",
   "metadata": {},
   "source": [
    "| Feature           | Lasso (L1)                       | Ridge (L2)                        |   |                      |\n",
    "| ----------------- | -------------------------------- | --------------------------------- | - | -------------------- |\n",
    "| Penalty           | ( \\lambda \\sum                   | w\\_i                              | ) | $\\lambda \\sum w_i^2$ |\n",
    "| Effect            | Shrinks some weights to **zero** | Shrinks all weights, none to zero |   |                      |\n",
    "| Feature Selection | **Yes** – leads to sparse models | No – all features retained        |   |                      |\n",
    "| Use Case          | When feature selection is needed | When multicollinearity exists     |   |                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7301f",
   "metadata": {},
   "source": [
    "58. Explain the concept of cross-validation and why it is used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950a9f1",
   "metadata": {},
   "source": [
    "Cross-validation is a technique to assess model generalization by dividing the data into multiple train-test splits.\n",
    "\n",
    "    K-Fold Cross-Validation:\n",
    "\n",
    "Data is split into k folds\n",
    "\n",
    "Each fold is used once as test, remaining as train\n",
    "\n",
    "Performance is averaged\n",
    "\n",
    "    Ensures reliable model evaluation, reduces overfitting risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ceef2a",
   "metadata": {},
   "source": [
    "59. What are some common evaluation metrics used for regression tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e453f3ab",
   "metadata": {},
   "source": [
    "Mean Absolute Error (MAE): Average of absolute differences\n",
    "\n",
    "Mean Squared Error (MSE): Average of squared differences\n",
    "\n",
    "Root Mean Squared Error (RMSE): Square root of MSE\n",
    "\n",
    "R-squared (R²): Proportion of variance explained\n",
    "\n",
    "Adjusted R²: R² adjusted for number of predictors\n",
    "\n",
    "    Helps compare model performance and select the best regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171208d2",
   "metadata": {},
   "source": [
    "60. How does the K-nearest neighbors (KNN) algorithm make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f539a",
   "metadata": {},
   "source": [
    "    For classification:\n",
    "\n",
    "Compute distance between query point and all training points.\n",
    "\n",
    "Select k-nearest neighbors.\n",
    "\n",
    "Return the most frequent class among neighbors (majority vote).\n",
    "\n",
    "    For regression:\n",
    "\n",
    "Compute distances.\n",
    "\n",
    "Select k-nearest.\n",
    "\n",
    "Return the average or median of target values.\n",
    "\n",
    "    No training phase → predictions are made based on stored training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c32d498",
   "metadata": {},
   "source": [
    "61. What is the curse of dimensionality, and how does it affect machine learning algorithms \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c92139",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to problems that arise when data has too many features (dimensions).\n",
    "\n",
    "    Effects:\n",
    "\n",
    "Data sparsity: More dimensions → more space → fewer data points in any region.\n",
    "\n",
    "Distance metrics lose meaning, affecting models like KNN or clustering.\n",
    "\n",
    "Overfitting increases, especially in models with high complexity.\n",
    "\n",
    "    Solution: Apply feature selection or dimensionality reduction techniques (like PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace4c308",
   "metadata": {},
   "source": [
    "62. What is feature scaling, and why is it important in machine learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c11253",
   "metadata": {},
   "source": [
    "Feature scaling transforms features so they fall within a similar range.\n",
    "\n",
    "    Importance:\n",
    "\n",
    "Prevents features with large values from dominating distance-based models (KNN, SVM, etc.).\n",
    "\n",
    "Helps gradient descent converge faster in optimization-based models.\n",
    "\n",
    "    Common methods:\n",
    "\n",
    "Min-Max Scaling: Scales values between 0 and 1\n",
    "\n",
    "Standardization: Centers around mean 0 with unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6006925",
   "metadata": {},
   "source": [
    "63. How does the Naïve Bayes algorithm handle categorical features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49479b9",
   "metadata": {},
   "source": [
    "Naïve Bayes handles categorical features using frequency-based probability estimation:\n",
    "\n",
    "It calculates:\n",
    "\n",
    "    P(feature value∣class)\n",
    "\n",
    "Probabilities are learned from the frequency of each feature-class combination in the training data.\n",
    "\n",
    "    Simple and efficient for text, categorical, and discrete data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bddd64c",
   "metadata": {},
   "source": [
    "64. Explain the concept of prior and posterior probabilities in Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b22b14",
   "metadata": {},
   "source": [
    "Prior Probability (P(C)): The probability of a class before observing the feature values.\n",
    "\n",
    "Posterior Probability (P(C∣X)): The updated probability of a class after seeing the data.\n",
    "\n",
    "    Bayes' Theorem:\n",
    "\n",
    "P(C∣X)= P(X∣C)⋅P(C)/P(X)\n",
    "\n",
    "    ​Used to predict the most probable class given the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c56f1",
   "metadata": {},
   "source": [
    "65. What is Laplace smoothing, and why is it used in Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a498d6c",
   "metadata": {},
   "source": [
    "Laplace Smoothing (also called add-one smoothing) is used to handle zero probabilities when a feature-category combination doesn't appear in training data.\n",
    "\n",
    "    P(xi∣y) = count(xi,y)+1/count(y)+k\n",
    "\n",
    "k = number of possible categories for the feature\n",
    "\n",
    "    Prevents probabilities from becoming zero, which would invalidate the entire product in Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb5e64c",
   "metadata": {},
   "source": [
    "66. Can Naïve Bayes handle continuous features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f188f9d",
   "metadata": {},
   "source": [
    "Yes. Gaussian Naïve Bayes is used for continuous features.\n",
    "\n",
    "Assumes features follow a normal distribution.\n",
    "\n",
    "Uses the probability density function (PDF):\n",
    "\n",
    "    P(x∣y)= 1/sq.rt.(2πσ^2)⋅e^(−(x−μ)^2/2σ^2)\n",
    " \n",
    "    Where:\n",
    "μ = mean of feature for class y\n",
    "\n",
    "σ = standard deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec689deb",
   "metadata": {},
   "source": [
    "67. What are the assumptions of the Naïve Bayes algorithm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a843eb",
   "metadata": {},
   "source": [
    "1. Feature Independence: All features are conditionally independent given the class.\n",
    "\n",
    "2. Equal importance: All features contribute equally and independently to the outcome.\n",
    "\n",
    "3. Class-conditional probability: Each feature follows a known probability distribution (like Gaussian for continuous).\n",
    "\n",
    "        Though often violated, Naïve Bayes still performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc7d5e",
   "metadata": {},
   "source": [
    "68. How does Naïve Bayes handle missing values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6650d04",
   "metadata": {},
   "source": [
    "    Approaches:\n",
    "\n",
    "Ignore missing features in the probability computation.\n",
    "\n",
    "Use probabilistic estimates based on known values.\n",
    "\n",
    "Apply imputation techniques (mean, mode, etc.) before training or prediction.\n",
    "\n",
    "    Naïve Bayes is relatively robust to missing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5812265",
   "metadata": {},
   "source": [
    "69. What are some common applications of Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c830ac98",
   "metadata": {},
   "source": [
    "Spam filtering\n",
    "\n",
    "Sentiment analysis\n",
    "\n",
    "Document classification\n",
    "\n",
    "Medical diagnosis\n",
    "\n",
    "Recommendation systems\n",
    "\n",
    "Credit scoring\n",
    "\n",
    "    Especially effective for text-based and high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe2e00",
   "metadata": {},
   "source": [
    "70. Explain the difference between generative and discriminative models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb799de",
   "metadata": {},
   "source": [
    "| Feature            | Generative Model                               | Discriminative Model                     |     |\n",
    "| ------------------ | ---------------------------------------------- | ---------------------------------------- | --- |\n",
    "| Learns             | Joint probability $P(X, Y)$                    | Conditional probability (P(Y             | X)) |\n",
    "| Goal               | How data is generated                          | How to separate/classify the data        |     |\n",
    "| Examples           | Naïve Bayes, Hidden Markov Models, GANs        | Logistic Regression, SVM, Decision Trees |     |\n",
    "| Can generate data? | Yes                                            | No                                       |     |\n",
    "| Use case           | Modeling full distribution or creating samples | Predicting labels accurately             |     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d99c7",
   "metadata": {},
   "source": [
    "71. How does the decision boundary of a Naïve Bayes classifier look like for binary classification tasks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e0f3d",
   "metadata": {},
   "source": [
    "    In binary classification, the decision boundary of Naïve Bayes is typically:\n",
    "\n",
    "Linear when using Gaussian Naïve Bayes with normally distributed features.\n",
    "\n",
    "Non-linear if distributions are non-Gaussian or features are not independent.\n",
    "\n",
    "The decision boundary is defined by where the posterior probabilities of both classes are equal:\n",
    "\n",
    "    P(C1∣X)=P(C2∣X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c2752c",
   "metadata": {},
   "source": [
    "72. What is the difference between multinomial Naïve Bayes and Gaussian Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bea30a",
   "metadata": {},
   "source": [
    "| Feature           | Multinomial Naïve Bayes                           | Gaussian Naïve Bayes                         |\n",
    "| ----------------- | ------------------------------------------------- | -------------------------------------------- |\n",
    "| Used For          | **Discrete/counted features** (e.g., word counts) | **Continuous features** (e.g., measurements) |\n",
    "| Assumes           | Features follow **multinomial distribution**      | Features follow **normal distribution**      |\n",
    "| Example Use Cases | Text classification, spam filtering               | Iris dataset, medical diagnosis              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf28480",
   "metadata": {},
   "source": [
    "73. How does Naïve Bayes handle numerical instability issues \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a645fa7",
   "metadata": {},
   "source": [
    "Naïve Bayes uses logarithmic probabilities to avoid underflow caused by multiplying many small probabilities:\n",
    "\n",
    "    Instead of:\n",
    "\n",
    "P(C∣X)=P(x1∣C)⋅P(x2∣C)⋅...⋅P(xn∣C)\n",
    "\n",
    "    It computes:\n",
    "\n",
    "logP(C∣X)=logP(C)+∑logP(xi∣C)\n",
    "\n",
    "    \n",
    "    Prevents numerical underflow and allows better handling of small probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6124456",
   "metadata": {},
   "source": [
    "74. What is the Laplacian correction, and when is it used in Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d7a82",
   "metadata": {},
   "source": [
    "    Laplacian correction (Laplace smoothing) is used to:\n",
    "\n",
    "Avoid zero probabilities for unseen feature-class combinations.\n",
    "\n",
    "Add a small constant (typically 1) to frequency counts:\n",
    "\n",
    "P(xi∣y)= count(xi,y)+1/count(y)+k\n",
    "\n",
    "    Used when dealing with categorical or sparse data, such as in text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf2852",
   "metadata": {},
   "source": [
    "75. Can Naïve Bayes be used for regression tasks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e8112",
   "metadata": {},
   "source": [
    "    Yes, but rarely. There's a variant called Naïve Bayes Regression, where:\n",
    "\n",
    "The target variable is continuous.\n",
    "\n",
    "Conditional distributions are assumed (e.g., Gaussian).\n",
    "\n",
    "    Not commonly used in practice because standard regression methods (linear regression, decision trees) often perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c1269",
   "metadata": {},
   "source": [
    "76. Explain the concept of conditional independence assumption in Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d00c94",
   "metadata": {},
   "source": [
    "The conditional independence assumption means:\n",
    "\n",
    "    P(x1,x2,...,xn∣C) = i=1∏n P(xi∣C)\n",
    "\n",
    "\n",
    "That is, all features are assumed to be independent of each other given the class label.\n",
    "\n",
    "    This simplifies computation significantly\n",
    "    \n",
    "    Often violated in practice, but Naïve Bayes still performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a2bfa",
   "metadata": {},
   "source": [
    "77. What are some drawbacks of the Naïve Bayes algorithm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78075bf",
   "metadata": {},
   "source": [
    "Strong independence assumption rarely holds in real-world data\n",
    "\n",
    "Performs poorly when features are highly correlated\n",
    "\n",
    "Cannot learn interactions between features\n",
    "\n",
    "Not ideal for small datasets with high feature cardinality\n",
    "\n",
    "Sensitive to imbalanced data without prior adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8360ff",
   "metadata": {},
   "source": [
    "78. Explain the concept of smoothing in Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa838b82",
   "metadata": {},
   "source": [
    "    Smoothing refers to techniques like Laplace (add-1) or Lidstone (add-α) smoothing that adjust probability estimates to:\n",
    "\n",
    "Avoid zero probabilities for unseen data\n",
    "\n",
    "Improve generalization\n",
    "\n",
    "Handle sparse datasets (common in text data)\n",
    "\n",
    "    Example: Instead of assigning 0 to unseen word frequency, give it a small non-zero value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd78c1",
   "metadata": {},
   "source": [
    "79. How does Naïve Bayes handle categorical features with a large number of categories \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f297d4b",
   "metadata": {},
   "source": [
    "    Challenges:\n",
    "\n",
    "Sparse probabilities\n",
    "\n",
    "Overfitting on rare categories\n",
    "\n",
    "Computational overhead\n",
    "\n",
    "    Solutions:\n",
    "\n",
    "Use Laplace smoothing\n",
    "\n",
    "Group rare categories as “Other”\n",
    "\n",
    "Apply feature hashing or embedding\n",
    "\n",
    "Use dimensionality reduction or frequency-based binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c6d0d",
   "metadata": {},
   "source": [
    "80. How does Naïve Bayes handle imbalanced datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd54b3",
   "metadata": {},
   "source": [
    "By default, Naïve Bayes can be biased toward the majority class.\n",
    "\n",
    "    Solutions:\n",
    "\n",
    "Use class priors to adjust probability toward minority class.\n",
    "\n",
    "Apply resampling techniques (oversample minority or undersample majority).\n",
    "\n",
    "Use SMOTE for synthetic sampling.\n",
    "\n",
    "Adjust decision threshold based on predicted probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
